[
    {
        "#MeToo in school_ teachers\u2019 and young learners\u2019 lived experience of verbal sexual harassment as a pedagogical opportunity.pdf": {
            "keywords": "sexual harassment, #MeToo, human rights education, recognition, intersectionality sexual harassment, #MeToo, human rights education, recognition, intersectionality",
            "abstract": "Based on a case study of verbal sexual harassment experienced by a young female teacher and her 17-year-old student in a Norwegian upper secondary school, this article addresses challenges and strengths of drawing upon negative experiences of 'lived injustice' in class, arguing that such experiences can serve as a resource for education about, through and for human rights. Complementing this case study, we discuss a survey we have conducted among secondary school students (N=382), concerning how young learners report being sexually harassed and how often they experience that an adult intervenes in the situation. Combining the theoretical framework of human rights education (HRE) and the concepts of intersectionality and recognition, this article discusses the pedagogical potential of drawing upon teachers' and young learners' experiences of verbal sexual harassment.",
            "introduction": "The #MeToo movement is part of a historical development of democratisation processes and the human struggle for recognition, because 'the desire for the state to recognize one's basic dignity has been at the core of democratic movements since the French revolution'",
            "conclusions": "The reasons behind the boys' sexual harassment of Mary and Ingunn are probably to be found in a combination of several intentional or unintentional motives. These range from a misplaced sexual desire to a way of cultivating a sense of group identity and belonging 'to the guys' through sexualised language or by stirring up power relations in class by challenging the young female teacher's privileged power position. One limitation of this study is that we only have Ingunn\u00b4s perspective.\nCould the harassment have been avoided if Ingunn had enforced strict rules in the classroom instead of preferring joking to strictness when the boys went 'too far'? And could it therefore be that the harassment was the result of a lack of leadership? The boys in class could be considered as marginalised youth at risk of early offending. Teacher-student relations are critical for influencing students' attitudes, especially when it is a matter of protecting at-risk teenagers from early offending"
        }
    },
    {
        "(KA)2_ building ontologies for the Internet_ a mid-term report.pdf": {
            "keywords": "",
            "abstract": "Ontologies are becoming increasingly more important in many di!erent areas, including the knowledge management area. In knowledge management, ontologies can be used as an instrument to make knowledge assets intelligently accessible to people in organizations through an Intranet or the Internet. Most enterprises agree that knowledge is an essential asset for success and survival on an increasingly competitive and global market. In this paper, we present an ontology-based approach through a large-scale initiative involving knowledge management for the knowledge-acquisition research community.",
            "introduction": "According to Information Week",
            "conclusions": "In this article, we presented a knowledge engineering approach to knowledge management, which is based on many years of experience in dealing with knowledge. If we relate our work to the four knowledge management actions mentioned in the introduction, we get the following.\nE Knowledge gathering is performed from existing HTML pages (knowledge annotation). E Knowledge organization and structuring is done through an ontology (ontological engineering). E Knowledge re\"nement is performed distributively by each worker (update annotations). E Knowledge distribution is done by a web crawler that gives intelligent access to the knowledge that is &&managed''. This is a pull approach where users take the initiative when they need knowledge. However, the work presented here could as well be used for a push approach.\nWe noted that knowledge management also involves people, and therefore any knowledge management e!ort is doomed to fail if human factors are not taken seriously. Knowledge management only works if people cooperate and are willing to share their (KA): BUILDING ONTOLOGIES FOR THE INTERNET knowledge. One way to stimulate sharing of knowledge is to change the incentive system accordingly. Another important social aspect is that, in order to be successful with a large-scale initiative, lightweight tools are a must.\nWe presented and discussed our approach to knowledge management through a large initiative for the knowledge-acquisition community concerned with ontology building on the Internet, knowledge annotation of web pages and query answering. The results so far highlight both the technical and social issues when dealing with knowledge management. We also discussed solution directions for some technical problems (e.g. tool support). Social issues concern above all people's willingness to participate in a knowledge management initiative, which, in a scienti\"c community, seems more feasible than in a commercial organization.\nOne of the aims of (KA) is to provide better and easy access to relevant information about knowledge acquisition. A simple alternative for this aim would be to provide a knowledge-acquisition meta-page with a collection of all kinds of relevant links for knowledge acquisition. Examples of such web sites in knowledge acquisition include the KAW mailing list (http://www.swi.psy.uva.nl/mailing-lists/kaw/home.html) and the KAW archives page at: http://ksi.cpsc.ucalgary.ca/KAW/. Indeed, for building the research topics ontology of knowledge acquisition, such web sites have been heavily used as useful sources of information. In the near future, we plan to extend the ontology to include also case-based reasoning."
        }
    },
    {
        ".879-approximation algorithms for MAX CUT and MAX 2SAT.pdf": {
            "keywords": "",
            "abstract": "We present randomized approximation algorithms for the MAX CUT and MAX 2SAT problems that always deliver solutions of expected value at least .87856 times the optimal value. These algorithms use a simple and elegant technique that randomly rounds the solution to a nonlinear programming relaxation. This relaxation can be interpreted both as a semidefinite program and as an eigenvalue minimization problem. We then show how to derandomize the algorithm to obtain approximation algorithms with the same performance guarantee of .87856. The previous best-known approximation algorithms for these problems had performance guarantees of ~for MAX CUT and ~for MAX 2SAT. A slight extension of our analysis leads to a .79607-approximation algorithm for the maximum directed cut problem, where a & approximation algorithm was the previous best-known algorithm. Our algorithm gives the first substantial progress in approximating MAX CUT in nearly twenty years, and, to the best of our knowledge, represents the first use of semidefinite programming in the design of approximation algorithms.",
            "introduction": "Given an undirected graph G = (V, 1?) and nonnegative weights wij = wli on the edges (i, j) c E, the maximum cut problem (MAX CUT) is that of finding the set of vertices S that maximizes the weight of the edges in the cut (S, ~); that is, the weight of the edges with one endpoint in S and the other in $. For simplicity, we usually set wij = O for (i, j) @ E and denote the weight of a cut (S, ~) by w(S, ~) = Ei=s,j$s wij.\nThe MAX CUT problem is one of the Karp's original NP-complete problems",
            "conclusions": ""
        }
    },
    {
        "2+p-SAT_ Relation of typical-case complexity to the nature of the phase transition.pdf": {
            "keywords": "",
            "abstract": "Heuristic methods for solution of problems in the NP-Complete class of decision problems often reach exact solutions, but fail badly at \"phase boundaries,\" across which the decision to be reached changes from almost always having one value to almost always having a different value. We report an analytic solution and experimental investigations of the phase transition that occurs in the limit of very large problems in K-SAT. The nature of its \"random first-order\" phase transition, seen at values of K large enough to make the computational cost of solving typical instances increase exponentially with problem size, suggests a mechanism for the cost increase. There has been evidence for features like the \"backbone\" of frozen inputs which characterizes the UNSAT phase in K-SAT in the study of models of disordered materials, but this feature and this transition are uniquely accessible to analysis in K-SAT. The random first-order transition combines properties of the 1st order (discontinuous onset of order) and 2nd order (with power law scaling, e.g. of the width of the critical region in a finite system) transitions known in the physics of pure solids. Such transitions should occur in other combinatoric problems in the large N limit. Finally, improved search heuristics may be developed when a \"backbone\" is known to exist.",
            "introduction": "The most natural continuation scheme, called Replica Symmetry (RS) has recently been shown to be exact at high temperature T [26,27] for the K-SAT model. Though not explicitly proven in",
            "conclusions": ""
        }
    },
    {
        "2K_ An Integrated Approach of QoS Compilation and Reconfigurable, Component-Based Run-Time Middleware for the Unified QoS Management Framework.pdf": {
            "keywords": "",
            "abstract": "Different distributed component-based applications (e.g., distributed multimedia, library information retrieval, secure stock trading applications), running in heterogeneous execution environments, need different quality of service (QoS). The semantics of QoS requirements and their provisions are application-specific, and they vary among different application domains. Furthermore, QoS provisions vary per applications in heterogeneous execution environments due to the varying distributed resource availability. Making these applications QoS-aware during the development phase, and ensuring their QoS guarantees during the execution phase is complex and hard. In this paper, we present a unified QoS management framework, called 2K Q+ . This framework extends our existing run-time 2K Q middleware system [1] by including our uniform QoS programming environment and our automated QoS compilation system (Q-Compiler). The uniform QoS programming and its corresponding QoS compilation allow and assist the application developer to build different component-based domain applications in QoS-aware fashion. Furthermore, this novel programming and compilation environment enables the applications to be instantiated, managed, and controlled by the same reconfigurable, component-based run-time middleware, such as 2K Q , in heterogeneous environments. Our experimental results show that different QoS-aware applications, using the 2K Q+ framework, get configured and setup fast and efficiently.",
            "introduction": "The rapid growth of distributed component-based environments and coexistence of different application domains, such as multimedia and electronic commerce, present significant challenges to the provision of different applications' Quality This work was supported by the National Science Foundation under contract numbers NSF EIA 9870736, NSF CCR-9988199, the Air Force Grant under contract number F30602-97-2-0121, NSF CISE Infrastructure grant under contract number NSF EIA 99-72884, and NASA grant under contract number NASA NAG 2-1250.\nof Service (QoS). First, different domain applications have specific semantics for QoS requirements and provisions. For example, multimedia and library information retrieval applications are concerned about the service qualities of audio, video streaming (e.g., frame rate, frame size, sampling rate, end-to-end delay) and messaging (e.g., priority, response time, reliability), respectively. Developing an application to be QoS-aware during the application development cycle, and ensuring its QoS provisions during the application's run-time are non-trivial tasks for a specific application and even harder for different applications. Second, with the pervasive computing, these applications are expected to be executed in heterogeneous computing and communication environments with different capacities of processing power (e.g., high performance PC, PDAs), battery power, and network bandwidth (e.g., wired, wireless networks). The big challenging problem, as shown in Fig.",
            "conclusions": ""
        }
    },
    {
        "3.2.3 An Approach to Design for Safety in Complex Systems.pdf": {
            "keywords": "",
            "abstract": "Most traditional hazard analysis techniques rely on discrete failure events that do not adequately handle software intensive systems or system accidents resulting from dysfunctional interactions between system components. This paper demonstrates a methodology where a hazard analysis based on the STAMP accident model is performed together with the system development process to design for safety in a complex system. Unlike traditional hazard analyses, this approach considers system accidents, organizational factors, and the dynamics of complex systems. The analysis is refined as the system design progresses and produces safety-related information to help systems engineers in making design decisions for complex safety-critical systems. The preliminary design of a Space Shuttle Thermal Tile Processing System is used to demonstrate the approach.",
            "introduction": "As the complexity of engineered systems increases, hazard analysis techniques have continued to lag behind the state-of-the-art engineering practice. Traditional event-based analyses consist of identifying the discrete failure events that could lead the system to a hazardous state. These events are usually organized into causal chains or trees. Popular eventbased hazard analysis techniques include Fault Tree Analysis (FTA) and Failure Modes and Effects Criticality Analysis",
            "conclusions": "This article described an approach to design for safety for complex systems that involves a hazard analysis process starting early in the system development phase and evolving throughout the system lifecycle. The approach is based on a new model of accident causation called STAMP that views safety as a dynamic control problem. In this context, the hazard analysis is used to provide the safety related information required by systems engineers to make tradeoff decisions during the design process. The technique was illustrated using a safety-critical system of a scope that allowed a walkthrough of the hazard analysis methodology. A single hazard was considered in this paper, and the analysis performed was nowhere near exhaustive. The example used did not include a complex socio-technical control structure. As such, complex modeling of the adaptation mechanisms of the socio-technical control structure was not necessary as it would be in a larger, more complex system. Nevertheless, the analysis offered a demonstration of how the STAMP accident modeling framework can be used to design for safety in complex systems."
        }
    },
    {
        "3D chainmail_ a fast algorithm for deforming volumetric objects.pdf": {
            "keywords": "",
            "abstract": "An algorithm is presentedthatenables fast deformation of volumetric objects. Using this algorithm, rigid, defm-rnable,elastic and plastic materialscan be modeled by adjusting deformation limits for individual elements. An interactivesystem thatcombines the deformation algorithm with collision deteetion and an energy minimizing elastic relaxation step is described. Using this system, objects containing up to-125,000 elements have b deformed intem~tivelyon an-SGI Indy.\n-.",
            "introduction": "Surgical simulation requires interactive modeling and visualization of complex, 3D anatomical structures. For example, surgery of the abdomen involves probing and cutting through organs and tissues that have complex shapes and material properties. Because modeling the deformation and cutting of tissue requires a representation of interior structure, volumetric object representations are well suited for surgical simulation. A volumetric representation can incorporate detailed information about internal anatomical or physiological structure. This detailed information can be used to model tissue deformation more accurately than models which represents the object surface and assumes a homogeneous interior. Because a volumetric representation uses the data produced by 3D medical scanners directly. errors that are introduced by fitting polygonal surfaces to the discrete image data can be avoided.\nIn a volumetric object representation, the object is stored as a discrete 3D army of sampled data elements. Each data element can consist of seve~bvtcs of information includ-. ing visual properties, such as color or tmnsparcncy,or material properties,such as tissue type or elasticity. The major disadvantageof volumetricrepmentations is that objects can consist of millions of volume elements. This large data requirement poses challenges for memory storage and access, for real-time rendering, and for physically realistic modeling of object interactions. In this paper we present a fast algorithm for modeling the deformation of volumetic objects. The algorithm can model a range of materials including rigid, deformable, elastic, and plastic substances. In addition, the method can model anisotmpic materials. such as muscle. which have different material properties along different &es.\nPermission to make digitallhard copies of all or pad of this material for personal or classroom use is granted without fee provided thnt the copies are not made or distributed for profit or commercial advantage. the copyright notice, the title of the publication msd its chte appear, and notice is given that copyright is by pem~ission of the ACM, Inc. To copy otherwise, to republish, to post on sewers or to redistribute to lists, requires specific permission sndlor fee.\nThe basic technologies that have irdluenced this work are: Volume Graphics; physics-based graphics; and soft-tissue modeling with Finite Element (FEM) and other methods.",
            "conclusions": "We have presented an rdgorithm that enables fast deformation of objects containing hundreds of thousands of volumetric elements. This algorithm can model a range of substances including; rigid, deformable, elastic, and plastic materials. Unlike other work where deformation is modeled with complex calculations on a small number of ekments, this algorithm performs simple calculations on a very large number of elements to achieve complex behavior."
        }
    },
    {
        "3D collision detection_ a survey.pdf": {
            "keywords": "",
            "abstract": "% D \u00a7 2 )! IH P ' )3 5 \u00a9 Q' (3 5% SR 8% # 5! UT WV X 4 6E 5Y &' ( \u00a7 U' ( \u00a9 8 \u00a7 ab% D 4 c% d' (2 \" e Ff I A F ' ) D! e g \u00a7 \u00a9 5# \u00a8% D! )E F% D h \u00a7 iV X 4 cE 5Y p' (% D2 Gb2 ( \u00a7 E 53 & D! q Fr p 4 6% s \u00a7 E 5E F% @ \u00a7 2 ' ( \u00a7 2 )% # b'\n' ) \u00a9 8 \u00a7 U &1 % 4 c% d' (2 u \u00a7 1 2 ) y' (3 54 6! t UT ' (% D\u00a9 6 \u00a7 ! \"! )Y 54 6% c \"1 % \u00a9 5% D2 ) \u00a7 8E C ! ) ' ) \u00a9 & gA 5Y &' 2 )% D \u00a7 yw ~H X 2 ) # 4 6 d# pw % ! t' )% D\u00a9 5# ' ) b3 8 \u00a7 @9 % I \u00a7 b ' t T F# 5% D1 % \u00a9 5% D2 ) \u00a7 D % D! W!",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "3D Finite Element Meshing from Imaging Data..pdf": {
            "keywords": "",
            "abstract": "This paper describes an algorithm to extract adaptive and quality 3D meshes directly from volumetric imaging data. The extracted tetrahedral and hexahedral meshes are extensively used in the Finite Element Method (FEM). A top-down octree subdivision coupled with the dual contouring method is used to rapidly extract adaptive 3D finite element meshes with correct topology from volumetric imaging data. The edge contraction and smoothing methods are used to improve the mesh quality. The main contribution is extending the dual contouring method to crack-free interval volume 3D meshing with feature sensitive adaptation. Compared to other tetrahedral extraction methods from imaging data, our method generates adaptive and quality 3D meshes without introducing any hanging nodes. The algorithm has been successfully applied to constructing the geometric model of a biomolecule in finite element calculations.",
            "introduction": "The development of finite element simulations in medicine, molecular biology and engineering has increased the need for high quality finite element meshes. Although there has been tremendous progresses in the area of surface reconstruction and 3D geometric modelling, it still remains a challenging process to generate 3D meshes directly from imaging data, such as Computed Tomography (CT), Magnetic Resonance Imaging (MRI) and Signed Distance Function (SDF). The imaging data V is given in the form of sampled function values on rectilinear grids, V = {F(i,j,k) | i, j, k are indices of x,y,z coordinates in a rectilinear grid.}. We assume a continuous function F is constructed through the trilinear interpolation of sampled values for each cubic cell in the volume.\nFor accurate and efficient finite element calculations, it is important to have adaptive and high quality geometric models with minimal number of elements. The studied object may have complicated topology. Figure",
            "conclusions": ""
        }
    },
    {
        "3D IBFV_ hardware-accelerated 3D flow visualization.pdf": {
            "keywords": "CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms Flow Visualization, Hardware Acceleration, Texture Advection, OpenGL",
            "abstract": "We present a hardware-accelerated method for visualizing 3D flow fields. The method is based on insertion, advection, and decay of dye. To this aim, we extend the texture-based IBFV technique presented in",
            "introduction": "Visualization of two and three dimensional vector data produced from application areas such as computational fluid dynamics (CFD) simulations, environmental sciences, and material engineering is a challenging task. Although not a closed subject, 2D vector field visualization can now be addressed by a comprehensive array of methods, such as hedgehog and glyph plots, stream lines and surfaces, topological decomposition, and texture-based methods. The last class of methods produces a \"dense\", texture-like image representing a flow field, such as spot noise and line integral convolution (LIC). For a comprehensive survey of these methods, see",
            "conclusions": ""
        }
    },
    {
        "3D modeling and tracking of human lip motions.pdf": {
            "keywords": "",
            "abstract": "We address the problem of tracking and reconstructing 3D human lip motions from a 2D view. This problem is challenging due both to the complex nature of lip motions and the minimal data available from a raw video stream of the face. We counter both of these di culties with statistical approaches. We rst build a p h ysically-based 3D model of lips and train it to cover only the subspace of lip motions. We t h e n t r a c k this model in video by nding the shape within the subspace that maximizes the posterior probability o f the model given the observed features. In this study, the features are the likelihoods of the lip and non-lip color classes: we iteratively derive forces from these values to apply to the physical model and converge to the nal solution. Because of the full 3D nature of the model, this framework allows us to track the lips from any head pose. In addition, because of the constraints imposed by the learned subspace of the model, we are able to accurately estimate the full 3D lip shape from the 2D view.",
            "introduction": "The lips are a critical factor in human communication: they are an important cue for speech recognition and are among the primary components of facial expressions. As a result, the detailed shape of the lips is an important input for systems that wish to observe, process, and code human communication. However, obtaining this input has proved to be quite di cult. When we are presented with a speaker's lips in a video stream, there is not a lot of visual information to work with. This is especially true given the complex ways in which lips can move. Furthermore, while the head is typically moving about in 3D, we h a ve a vailable only the 2D projection. Even the contours of the lips are often obscured by the lighting, facial hair, or other disturbances. Among the only features we can depend on is the color content of the lips and surrounding regions, though even this information is often noisy.\nOur goal is to nd a means of robustly estimating the 3D lip shape despite these limitations. It has been clear from the beginning of our work that in order to reach this goal, we w ould need a 3D model so that the lips could be tracked from any pose. In addition, we realized that this model would have t o conform to a strict subspace that would contain only the permissible lip shapes. Without these constraints on the shape, the model could never estimate the full 3D shape from only noisy 2D information. Lastly, w e w ould like this model to be physically based so that we can apply the standard tools of deformation and equilibrium to drive i t f r o m l o c a l o b s e r v ations. To satisfy these needs, we h a ve constructed an FEM model of the lips and trained it with 3D data to conform to the subspace of lip motions.\nEven with this model, we are left with the formidable challenge of tracking the lips in video data. Again we c hoose a statistical approach, modeling the color distributions of the lip and non-lip regions. We are then able to compute likelihood maps of each class over the relevant region of the input stream. By then projecting our 3D model into the 2D camera view, we can measure the posterior probability of the model shape using this information. This method allows us to nd the correct lip pose by matching the distributions of the observations and those implied by the model, thereby maximizing the posterior probability of the model shape given the observations.\nIn this paper, we will describe in detail how w e h a ve formulated each of these techniques and how w e apply them to this problem. We will then show our results, demonstrating how o u r statistical framework has allowed to us accurately and robustly estimate and reconstruct the 3D lip shape from 2D video data.\nIn looking at the prior work on lip modeling, there are two major groups of models. The rst of these contains the models developed for analysis, usually intended for input into a combined audio-visual speech recognition system. The underlying assumption behind most of these models is that the head will be viewed from only one known pose. As a result, these models are only two-dimensional. Many are based directly on image data 5], 7] others use such l o w level features to form a parametrized description of the lip shape 1]. Some of the most interesting work done in this area has been in using a statistically trained model of lip variations. Bregler and Omohundro's work and Luettin's work, for example",
            "conclusions": "We h a ve presented a method for estimating and reconstructing the 3D shape of human lips from raw video data. We h a ve shown how w e can accurately match the observations in raw data, and have also demonstrated the ability of our model to accurately reconstruct 3D shapes from sparse 2D data. We h a ve achieved these goals by using a statistical approach throughout, Data Used 3D Reconstruction Error x y z ( 8 p o i n ts) from modeling the subspace of lip motions to describing and tting the observations in the video stream.\nThere are a number of directions in which w e wish to continue this work. Foremost among these is integrating a 3D head pose estimate with the tracking algorithm so that the tracking can be robust to arbitrary changes in pose. We also plan to evaluate how m uch new information is provided by a 3 D e stimate for tasks such as audio-visual speech recognition and facial expression recognition."
        }
    },
    {
        "A 3D-hole closing algorithm.pdf": {
            "keywords": "",
            "abstract": "Contrary to the 2D case, a 3D hole is not a subset of the 3D space. It is therefore not possible to use connected component search algorithms for detecting and suppressing 3D holes. In this paper, we propose an algorithm for closing 3D holes. It is based on properties of the previously introduced notion of topological numbers. Our algorithm is linear in time and it allows to control the size of the holes which are closed. As far as we know, this is the first 3D-hole closing algorithm.",
            "introduction": "In 3D image analysis, the reconstructed objects which result from the segmentation process have sometimes unwanted holes. These holes may be considered as an effect of noise. In many applications, the noisy holes can be characterized by their size: larger holes are actually features of the object, whereas a large amount of small holes are irrelevant.\nWe present in this paper a 3D-hole closing algorithm, which original implementation allows to control the size of the holes that are closed. The closing of the hole is actually performed by building a surface such as a kind of patch which obstructs the hole. As far as we know, this is the first 3D-hole closing algorithm.",
            "conclusions": "A 3D-hole closing algorithm has been presented. It is based on some properties of the topological numbers and on the principle of the topological hull. It has the originality to close the 3D-holes of different sizes in order to preserve those which are not actually resulting from a noise effect. In addition to that, our algorithm has the advantage to be linear in time complexity.\nThe method that we have proposed fits into the topological image processing domain. This domain has a large range of applications, such as the biomedical area (see"
        }
    },
    {
        "A ballistic model of choice response time..pdf": {
            "keywords": "",
            "abstract": "",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "A Bayesian framework for regularization.pdf": {
            "keywords": [],
            "abstract": "",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "A Bayesian Method for Fitting Parametric and Nonparametric Models to Noisy Data.pdf": {
            "keywords": [],
            "abstract": "",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "A Bayesian network framework for relational shape matching.pdf": {
            "keywords": "",
            "abstract": "A Bayesian network formulation for relational shape matching is presented. The main advantage of the relational shape matching approach is the obviation of the non-rigid spatial mappings used by recent non-rigid matching approaches. The basic variables that need to be estimated in the relational shape matching objective function are the global rotation and scale and the local displacements and correspondences. The new Bethe free energy approach is used to estimate the pairwise correspondences between links of the template graphs and the data. The resulting framework is useful in both registration and recognition contexts. Results are shown on hand-drawn templates and on 2D transverse T1-weighted MR images.",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "Biometric verification based on grip-pattern recognition.pdf": {
            "keywords": "Biometric verification, likelihood ratio, smart gun, grip-pattern recognition",
            "abstract": "This paper describes the design, implementation and evaluation of a user-verification system for a smart gun, which is based on grip-pattern recognition. An existing pressure sensor consisting of an array of 44 \u00d7 44 piezoresistive elements is used to measure the grip pattern. An interface has been developed to acquire pressure images from the sensor. The values of the pixels in the pressure-pattern images are used as inputs for a verification algorithm, which is currently implemented in software on a PC. The verification algorithm is based on a likelihoodratio classifier for Gaussian probability densities. First results indicate that it is feasible to use grip-pattern recognition for biometric verification.",
            "introduction": "Nowadays there is a growing interest in personalized applications that use biometrics as an access key. Wellknown methods use fingerprints, hand geometry, iris scans, or voice characteristics to identify a person or to verify a person's identity. Since technology is improving and becoming more affordable, biometrics is becoming more popular for daily use. Powerful processors provide the possibility of doing complex calculations on large sets of data within a short time. This creates new possibilities for high-speed verification or even identification for many everyday applications. This paper describes part of the development of a security system for a personalized handgun, a so-called 'smart gun', that makes use use of biometric verification. The biometric features that are used are those of the twodimensional pattern of the pressure that is exerted on the gun's butt. This pressure pattern will be further referred to as the grip pattern. The goal of the ongoing research is to contribute to a weapon that can only be fired by the rightful user.\nThe smart-gun concept receives great interest in the US, where weapon safety is an important issue. The technology described here might help to prevent many accidents at home, where young children get to play with their parent's guns.",
            "conclusions": "The current hardware implementation has proven to be useful for the first experiments and demonstrations. The piezo-resistive sensor array of the Tekscan sensor has been found suitable for detecting handgrip squeeze patterns and appeared to be a good option for a low-cost experimental setup. The current system uses a PC for the implementation of the verification algorithm. Though the training of the system requires some extensive computations, the verification part is quite straightforward and suitable for an efficient hardware implementation.\nThe test results indicate that the grip pattern contains sufficient information that can be used for verification. The current values for the EERs are not precise enough to make a well-founded statement about the performance of the system. This is caused by the limited number of data that was collected for training and testing.\nFurther improvements can be expected from a better modelling of the within-class covariance matrix of the intended user group, i.e. the police. This is based on the observation that experienced marksmen show very little variation in the way the hold their weapon. It is also worth investigating a further dimension reduction to a value below the number of users. This is likely to improve the performance on the test set and thus in the real world."
        }
    },
    {
        "[Re] Weighted Voronoi Stippling.pdf": {
            "keywords": "",
            "abstract": "",
            "introduction": "Adrian Secord introduces in",
            "conclusions": "Most of the results have been replicated even though some slight discrepancies remain in the final output for one image. It is difficult to identify the precise cause but most likely, the problem occurs because of the limited resolution of the input picture. We therefore think most of the results have been replicated."
        }
    },
    {
        "_Exhibitionists_ and _Voyeurs_ Do It Better_ A Shared Environment for Flexible Coordination with Tacit Messages.pdf": {
            "keywords": [],
            "abstract": "",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "_Language-Based Security_.pdf": {
            "keywords": "",
            "abstract": "",
            "introduction": "",
            "conclusions": ""
        }
    },
    {
        "_Perspective shape from shading_ and viscosity solutions.pdf": {
            "keywords": "",
            "abstract": "This article proposes a solution of the Lambertian shape from shading (SFS) problem in the case of a pinhole camera model (performing a perspective projection). Our approach is based upon the notion of viscosity solutions of Hamilton-Jacobi equations. This approach allows us to naturally deal with nonsmooth solutions and provides a mathematical framework for proving correctness of our algorithms. Our work extends previous work in the area in three aspects. First, it models the camera as a pinhole whereas most authors assume an orthographic projection (see",
            "introduction": "SFS has been a central problem in the field of computer vision since the early days. The problem is to compute the three-dimensional shape of a surface from the brightness variations in a grey level image of that surface. The work in our field was pioneered by Horn who was the first to pose the problem as that of finding the solution of a nonlinear first-order PDE called the brightness equation",
            "conclusions": "We have proposed a new method for recovering the shape of Lambertian objects from shaded images under perspective projection. This approach is based on a rigorous mathematical analysis; we have given results about the existence and uniqueness of a viscosity solution, provided an approximation scheme and a numerical algorithm for computing this solution. Finally, we have proved the convergence (with adequate hypotheses) of our numerical approximations toward the viscosity solution of our PDE.\nWe are extending our approach for recovering non Lambertian surfaces and for removing the requirement for boundary conditions."
        }
    }
]